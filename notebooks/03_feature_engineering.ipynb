{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-18T17:49:24.940030Z",
     "start_time": "2025-09-18T17:49:24.935736Z"
    }
   },
   "source": [
    "# StackOverflow Developer Survey - Feature Engineering\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T17:49:25.888064Z",
     "start_time": "2025-09-18T17:49:25.354460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Feature Engineering for Linear Regression Model\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load the cleaned and encoded dataset\n",
    "print(\"Loading the dataset...\")\n",
    "df_encoded = pd.read_csv('../data/processed/cleaned_survey_data.csv')\n",
    "print(\"Dataset loaded successfully.\")\n",
    "print(f\"Initial dataset shape: {df_encoded.shape}\")"
   ],
   "id": "2da45ad4d6ed3545",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering for Linear Regression Model\n",
      "==================================================\n",
      "Loading the dataset...\n",
      "Dataset loaded successfully.\n",
      "Initial dataset shape: (23928, 449)\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T17:49:26.009152Z",
     "start_time": "2025-09-18T17:49:26.006208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Create Career Satisfaction Features\n",
    "print(\"\\n1. Creating Career Satisfaction Features...\")"
   ],
   "id": "d3c098e2009f28e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Creating Career Satisfaction Features...\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T17:49:26.203847Z",
     "start_time": "2025-09-18T17:49:26.199830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_semicolon_items(series):\n",
    "    \"\"\"Count number of items in semicolon-seperated string\"\"\"\n",
    "    def count_items(x):\n",
    "        if pd.isna(x) or x == '' or str(x).lower() == 'nan':\n",
    "            return 0\n",
    "        return len([item.strip() for item in str(x).split(';') if item.strip()])\n",
    "    return series.apply(count_items)"
   ],
   "id": "6a3db4c0226bc547",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T17:49:26.640155Z",
     "start_time": "2025-09-18T17:49:26.633895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_satisfaction_feature(have_col, want_col, feature_name):\n",
    "    \"\"\"\n",
    "    Create satisfaction feature comparing what they have vs what they want to work with\n",
    "    :return: 0 = same (satisfied), 1 = want more (growth mindset), -1 = want less (simplification)\n",
    "    \"\"\"\n",
    "    have_counts = count_semicolon_items(df_encoded[have_col])\n",
    "    want_counts = count_semicolon_items(df_encoded[want_col])\n",
    "    \n",
    "    # Create the comparison feature\n",
    "    satisfaction = np.where(want_counts > have_counts, 1, # Want more (growth)\n",
    "                            np.where(want_counts < have_counts, -1, # Want less (simplification)\n",
    "                                     0)) # Same (satisfied)\n",
    "    \n",
    "    # Create additional metric\n",
    "    difference = want_counts - have_counts\n",
    "    ratio = np.where(have_counts > 0, want_counts / have_counts,\n",
    "                     np.where(want_counts > 0, 2, 1)) # handle division by zero\n",
    "    \n",
    "    print(f\"{feature_name}:\")\n",
    "    print(f\"  Growth mindset (want more): {(satisfaction == 1).sum()} ({(satisfaction == 1).mean()*100:.1f}%)\")\n",
    "    print(f\"  Satisfied (same amount): {(satisfaction == 0).sum()} ({(satisfaction == 0).mean()*100:.1f}%)\")\n",
    "    print(f\"  Simplification (want less): {(satisfaction == -1).sum()} ({(satisfaction == -1).mean()*100:.1f}%)\")\n",
    "    \n",
    "    return satisfaction, difference, ratio"
   ],
   "id": "5ec36928bdfb6ca7",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T17:49:27.245389Z",
     "start_time": "2025-09-18T17:49:27.092071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create satisfaction features for each domain\n",
    "satisfaction_features = {}\n",
    "\n",
    "# Languages\n",
    "if 'LanguageHaveWorkedWith' in df_encoded.columns and 'LanguageWantToWorkWith' in df_encoded.columns:\n",
    "    lang_satisfaction, lang_diff, lang_ratio = create_satisfaction_feature(\n",
    "        'LanguageHaveWorkedWith', 'LanguageWantToWorkWith', 'Languages'\n",
    "    )\n",
    "    satisfaction_features.update({\n",
    "        'Language_Satisfaction': lang_satisfaction,\n",
    "        'Language_Difference': lang_diff,\n",
    "        'Language_Ratio': lang_ratio\n",
    "    })\n",
    "    \n",
    "# Databases\n",
    "if 'DatabaseHaveWorkedWith' in df_encoded.columns and 'DatabaseWantToWorkWith' in df_encoded.columns:\n",
    "    db_satisfaction, db_diff, db_ratio = create_satisfaction_feature(\n",
    "        'DatabaseHaveWorkedWith', 'DatabaseWantToWorkWith', 'Databases'\n",
    "    )\n",
    "    satisfaction_features.update({\n",
    "        'Database_Satisfaction': db_satisfaction,\n",
    "        'Database_Difference': db_diff,\n",
    "        'Database_Ratio': db_ratio\n",
    "    })\n",
    "\n",
    "# Communication Platforms\n",
    "if 'CommPlatformHaveWorkedWith' in df_encoded.columns and 'CommPlatformWantToWorkWith' in df_encoded.columns:\n",
    "    comm_satisfaction, comm_diff, comm_ratio = create_satisfaction_feature(\n",
    "        'CommPlatformHaveWorkedWith', 'CommPlatformWantToWorkWith', 'Communication Platforms'\n",
    "    )\n",
    "    satisfaction_features.update({\n",
    "        'CommPlatform_Satisfaction': comm_satisfaction,\n",
    "        'CommPlatform_Difference': comm_diff,\n",
    "        'CommPlatform_Ratio': comm_ratio\n",
    "    })\n",
    "    \n",
    "# Add satisfaction features to dataframe\n",
    "for feature_name, feature_values in satisfaction_features.items():\n",
    "    df_encoded[feature_name] = feature_values\n",
    "    \n",
    "print(f\"Created {len(satisfaction_features)} career satisfaction features\")"
   ],
   "id": "e7b584915273892d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages:\n",
      "  Growth mindset (want more): 4664 (19.5%)\n",
      "  Satisfied (same amount): 6237 (26.1%)\n",
      "  Simplification (want less): 13027 (54.4%)\n",
      "Databases:\n",
      "  Growth mindset (want more): 3602 (15.1%)\n",
      "  Satisfied (same amount): 11395 (47.6%)\n",
      "  Simplification (want less): 8931 (37.3%)\n",
      "Communication Platforms:\n",
      "  Growth mindset (want more): 1173 (4.9%)\n",
      "  Satisfied (same amount): 11355 (47.5%)\n",
      "  Simplification (want less): 11400 (47.6%)\n",
      "Created 9 career satisfaction features\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T17:49:27.739236Z",
     "start_time": "2025-09-18T17:49:27.695174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Remove semicolon-separated string columns\n",
    "print(\"\\n2. Identifying and removing semicolon-separated string columns...\")\n",
    "\n",
    "# identify columns with semicolon-separated strings\n",
    "semicolon_cols = []\n",
    "for col in df_encoded.columns:\n",
    "    if df_encoded[col].dtype == 'object':\n",
    "        # Check if column contains semicolons\n",
    "        sample_values = df_encoded[col].dropna().head(100)\n",
    "        if any(';' in str(val) for val in sample_values):\n",
    "            semicolon_cols.append(col)\n",
    "            \n",
    "print(f\"Found {len(semicolon_cols)} semicolon-separated columns:\")\n",
    "for col in semicolon_cols:\n",
    "    print(f\"  -  {col}\")\n",
    "    \n",
    "# Remove these columns\n",
    "df_features = df_encoded.drop(columns=semicolon_cols)\n",
    "print(f\"Removed {len(semicolon_cols)} semicolon-separated columns\")\n",
    "print(f\"Dataset shape after removal: {df_features.shape}\")"
   ],
   "id": "b96b710c449883c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Identifying and removing semicolon-separated string columns...\n",
      "Found 14 semicolon-separated columns:\n",
      "  -  EmploymentAddl\n",
      "  -  LearnCode\n",
      "  -  LanguageHaveWorkedWith\n",
      "  -  LanguageWantToWorkWith\n",
      "  -  LanguageAdmired\n",
      "  -  DatabaseHaveWorkedWith\n",
      "  -  DatabaseWantToWorkWith\n",
      "  -  PlatformHaveWorkedWith\n",
      "  -  WebframeHaveWorkedWith\n",
      "  -  OpSysPersonal use\n",
      "  -  OpSysProfessional use\n",
      "  -  CommPlatformHaveWorkedWith\n",
      "  -  CommPlatformWantToWorkWith\n",
      "  -  SO_Dev_Content\n",
      "Removed 14 semicolon-separated columns\n",
      "Dataset shape after removal: (23928, 444)\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T17:49:28.472825Z",
     "start_time": "2025-09-18T17:49:28.368987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Handle remaining categorical variables\n",
    "print(\"\\n3. Processing remaining categorical variables...\")\n",
    "\n",
    "# Identify remaining object columns\n",
    "object_cols = df_features.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Remaining object columns: {len(object_cols)}\")\n",
    "\n",
    "# Handle remaining categorical columns\n",
    "for col in object_cols:\n",
    "    unique_vals = df_features[col].nunique()\n",
    "    print(f\"  {col}: {unique_vals} unique values\")\n",
    "    \n",
    "    if unique_vals <= 10:  # Low cardinality - one-hot encode\n",
    "        dummies = pd.get_dummies(df_features[col], prefix=col, drop_first=True)\n",
    "        df_features = pd.concat([df_features.drop(columns=[col]), dummies], axis=1)\n",
    "    else:  # High cardinality - frequency encoding\n",
    "        freq_encoding = df_features[col].value_counts()\n",
    "        df_features[f'{col}_frequency'] = df_features[col].map(freq_encoding)\n",
    "        df_features = df_features.drop(columns=[col])\n",
    "\n",
    "print(f\"Dataset shape after categorical encoding: {df_features.shape}\")"
   ],
   "id": "54fb4f723b0d042a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Processing remaining categorical variables...\n",
      "Remaining object columns: 3\n",
      "  DevType: 32 unique values\n",
      "  Industry: 16 unique values\n",
      "  Country: 164 unique values\n",
      "Dataset shape after categorical encoding: (23928, 444)\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T17:49:29.148091Z",
     "start_time": "2025-09-18T17:49:29.022796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5. Feature scaling and preparation for Linear Regression\n",
    "print(\"\\n5. Preparing features for Linear Regression...\")\n",
    "\n",
    "# Separate target variable\n",
    "target_col = 'ConvertedCompYearly'\n",
    "if target_col in df_features.columns:\n",
    "    y = df_features[target_col]\n",
    "    X = df_features.drop(columns=[target_col])\n",
    "    \n",
    "    print(f\"Features shape: {X.shape}\")\n",
    "    print(f\"Target shape: {y.shape}\")\n",
    "else:\n",
    "    print(f\"Error: Target column '{target_col}' not found!\")\n",
    "    print(\"Available columns:\", df_features.columns.tolist()[:10], \"...\")\n",
    "    # Exit early if target column not found\n",
    "    raise ValueError(f\"Target column '{target_col}' not found in dataset\")\n",
    "\n",
    "# Check for infinite values\n",
    "inf_cols = []\n",
    "for col in X.select_dtypes(include=[np.number]).columns:\n",
    "    if np.isinf(X[col]).any():\n",
    "        inf_cols.append(col)\n",
    "        X[col] = X[col].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "if inf_cols:\n",
    "    print(f\"Replaced infinite values in {len(inf_cols)} columns with NaN\")\n",
    "    # Fill the NaN values created from infinite values\n",
    "    X = X.fillna(X.median())\n",
    "\n",
    "# Remove constant columns (zero variance)\n",
    "constant_cols = []\n",
    "for col in X.columns:\n",
    "    if X[col].std() == 0:\n",
    "        constant_cols.append(col)\n",
    "\n",
    "if constant_cols:\n",
    "    X = X.drop(columns=constant_cols)\n",
    "    print(f\"Removed {len(constant_cols)} constant columns\")"
   ],
   "id": "11758df88bb9b676",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Preparing features for Linear Regression...\n",
      "Features shape: (23928, 443)\n",
      "Target shape: (23928,)\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T17:50:05.178663Z",
     "start_time": "2025-09-18T17:50:00.881089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 6. Feature selection\n",
    "print(\"\\n6. Feature selection...\")\n",
    "\n",
    "# Remove highly correlated features\n",
    "print(\"Removing highly correlated features...\")\n",
    "numeric_features = X.select_dtypes(include=[np.number])\n",
    "corr_matrix = numeric_features.corr().abs()\n",
    "\n",
    "# Find pairs of highly correlated features\n",
    "upper_triangle = corr_matrix.where(\n",
    "    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    ")\n",
    "\n",
    "# Find features with correlation > 0.95\n",
    "high_corr_features = [\n",
    "    column for column in upper_triangle.columns \n",
    "    if any(upper_triangle[column] > 0.95)\n",
    "]\n",
    "\n",
    "if high_corr_features:\n",
    "    X = X.drop(columns=high_corr_features)\n",
    "    print(f\"Removed {len(high_corr_features)} highly correlated features\")\n",
    "\n",
    "print(f\"Final feature set shape: {X.shape}\")"
   ],
   "id": "867a5cd893d0d99e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. Feature selection...\n",
      "Removing highly correlated features...\n",
      "Removed 3 highly correlated features\n",
      "Final feature set shape: (23928, 440)\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T17:50:30.967295Z",
     "start_time": "2025-09-18T17:50:30.549116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 7. Feature scaling\n",
    "print(\"\\n7. Feature scaling...\")\n",
    "\n",
    "# Use RobustScaler for better handling of outliers\n",
    "scaler = RobustScaler()\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "# Split data first to prevent data leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Fit scaler on training data only\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrames\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_names, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_names, index=X_test.index)\n",
    "\n",
    "print(\"Features scaled using RobustScaler\")\n"
   ],
   "id": "450ee7b17293ca19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. Feature scaling...\n",
      "Training set: (19142, 440)\n",
      "Test set: (4786, 440)\n",
      "Features scaled using RobustScaler\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T17:50:55.273218Z",
     "start_time": "2025-09-18T17:50:55.159899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 8. Final feature selection using statistical tests\n",
    "print(\"\\n8. Final feature selection...\")\n",
    "\n",
    "# Use SelectKBest with f_regression for feature selection\n",
    "n_features = min(50, X_train_scaled.shape[1])  # Select top 50 features or all if less\n",
    "selector = SelectKBest(score_func=f_regression, k=n_features)\n",
    "\n",
    "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features = X_train_scaled.columns[selector.get_support()].tolist()\n",
    "feature_scores = selector.scores_[selector.get_support()]\n",
    "\n",
    "print(f\"Selected {len(selected_features)} best features\")\n",
    "print(\"Top 10 features by F-score:\")\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'f_score': feature_scores\n",
    "}).sort_values('f_score', ascending=False)\n",
    "\n",
    "print(feature_importance_df.head(10))\n",
    "\n",
    "# Convert selected features back to DataFrames\n",
    "X_train_final = pd.DataFrame(X_train_selected, columns=selected_features, index=X_train.index)\n",
    "X_test_final = pd.DataFrame(X_test_selected, columns=selected_features, index=X_test.index)"
   ],
   "id": "40d96cd6f73ae0ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8. Final feature selection...\n",
      "Selected 50 best features\n",
      "Top 10 features by F-score:\n",
      "                                              feature      f_score\n",
      "49                                  Country_frequency  7290.733241\n",
      "1                                           YearsCode  3247.874148\n",
      "0                                             WorkExp  2641.358279\n",
      "41                 SODuration_Between 10 and 15 years   943.552343\n",
      "45  SODuration_More than 15 years, or since Stack ...   897.611304\n",
      "42                   SODuration_Between 3 and 5 years   816.722427\n",
      "31                                 Employment_Student   644.583733\n",
      "17                   PlatformHaveWorkedWith_Terraform   639.048193\n",
      "28                                Age_35-44 years old   603.433936\n",
      "14                    PlatformHaveWorkedWith_Homebrew   581.184334\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T17:51:19.298662Z",
     "start_time": "2025-09-18T17:51:19.290854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 9. Summary and export\n",
    "print(\"\\n9. Feature Engineering Summary\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Original dataset shape: {df_encoded.shape}\")\n",
    "print(f\"After removing semicolon columns: {df_features.shape}\")\n",
    "print(f\"After preprocessing: {X.shape}\")\n",
    "print(f\"Final training features: {X_train_final.shape}\")\n",
    "print(f\"Final test features: {X_test_final.shape}\")\n",
    "print(f\"Career satisfaction features created: {len(satisfaction_features)}\")\n",
    "print(f\"Semicolon columns removed: {len(semicolon_cols)}\")"
   ],
   "id": "be2f16833aa5c4d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9. Feature Engineering Summary\n",
      "========================================\n",
      "Original dataset shape: (23928, 458)\n",
      "After removing semicolon columns: (23928, 444)\n",
      "After preprocessing: (23928, 440)\n",
      "Final training features: (19142, 50)\n",
      "Final test features: (4786, 50)\n",
      "Career satisfaction features created: 9\n",
      "Semicolon columns removed: 14\n",
      "\n",
      "Career Satisfaction Feature Summary:\n",
      "Language_Satisfaction: Mean = -0.350, Std = 0.786\n",
      "Language_Difference: Mean = -1.519, Std = 3.470\n",
      "Language_Ratio: Mean = 0.839, Std = 0.751\n",
      "Database_Satisfaction: Mean = -0.223, Std = 0.689\n",
      "Database_Difference: Mean = -0.610, Std = 2.278\n",
      "Database_Ratio: Mean = 0.961, Std = 0.773\n",
      "CommPlatform_Satisfaction: Mean = -0.427, Std = 0.585\n",
      "CommPlatform_Difference: Mean = -1.356, Std = 2.264\n",
      "CommPlatform_Ratio: Mean = 0.781, Std = 0.393\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T17:52:10.796239Z",
     "start_time": "2025-09-18T17:52:10.789955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display career satisfaction feature distributions\n",
    "print(\"\\nCareer Satisfaction Feature Summary:\")\n",
    "for feature in satisfaction_features.keys():\n",
    "    if feature in df_encoded.columns:\n",
    "        print(f\"{feature}: Mean = {df_encoded[feature].mean():.3f}, Std = {df_encoded[feature].std():.3f}\")"
   ],
   "id": "f15e305028957b93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Career Satisfaction Feature Summary:\n",
      "Language_Satisfaction: Mean = -0.350, Std = 0.786\n",
      "Language_Difference: Mean = -1.519, Std = 3.470\n",
      "Language_Ratio: Mean = 0.839, Std = 0.751\n",
      "Database_Satisfaction: Mean = -0.223, Std = 0.689\n",
      "Database_Difference: Mean = -0.610, Std = 2.278\n",
      "Database_Ratio: Mean = 0.961, Std = 0.773\n",
      "CommPlatform_Satisfaction: Mean = -0.427, Std = 0.585\n",
      "CommPlatform_Difference: Mean = -1.356, Std = 2.264\n",
      "CommPlatform_Ratio: Mean = 0.781, Std = 0.393\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T17:54:08.802157Z",
     "start_time": "2025-09-18T17:54:08.424267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save processed data for modeling notebook\n",
    "print(\"\\n10. Saving processed data...\")\n",
    "\n",
    "# Save the processed datasets\n",
    "X_train_final.to_csv('../data/processed/X_train_processed.csv', index=False)\n",
    "X_test_final.to_csv('../data/processed/X_test_processed.csv', index=False)\n",
    "y_train.to_csv('../data/processed/y_train.csv', index=False)\n",
    "y_test.to_csv('../data/processed/y_test.csv', index=False)\n",
    "\n",
    "# Save feature names and scaler for future use\n",
    "import joblib\n",
    "joblib.dump(scaler, '../data/processed/feature_scaler.pkl')\n",
    "joblib.dump(selector, '../data/processed/feature_selector.pkl')\n",
    "\n",
    "# Save feature engineering metadata\n",
    "metadata = {\n",
    "    'selected_features': selected_features,\n",
    "    'career_satisfaction_features': list(satisfaction_features.keys()),\n",
    "    'removed_semicolon_columns': semicolon_cols,\n",
    "    'feature_importance': feature_importance_df.to_dict('records'),\n",
    "    'scaling_method': 'RobustScaler',\n",
    "    'feature_selection_method': f'SelectKBest_f_regression_top_{n_features}'\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../data/processed/feature_engineering_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"Feature engineering complete!\")\n",
    "print(\"Files saved:\")\n",
    "print(\"  - X_train_processed.csv\")\n",
    "print(\"  - X_test_processed.csv\") \n",
    "print(\"  - y_train.csv\")\n",
    "print(\"  - y_test.csv\")\n",
    "print(\"  - feature_scaler.pkl\")\n",
    "print(\"  - feature_selector.pkl\")\n",
    "print(\"  - feature_engineering_metadata.json\")\n",
    "print(\"\\nReady for modeling notebook!\")"
   ],
   "id": "b170cac2600608",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10. Saving processed data...\n",
      "Feature engineering complete!\n",
      "Files saved:\n",
      "  - X_train_processed.csv\n",
      "  - X_test_processed.csv\n",
      "  - y_train.csv\n",
      "  - y_test.csv\n",
      "  - feature_scaler.pkl\n",
      "  - feature_selector.pkl\n",
      "  - feature_engineering_metadata.json\n",
      "\n",
      "Ready for modeling notebook!\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2c7cd5495175a9ca"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
